{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f093b056-f11d-4555-95e0-0fd74d8db225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import RichProgressBar\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from dbtk.data.datasets import SequenceDataset\n",
    "from dbtk.nn.layers import TransformerEncoder, TransformerEncoderBlock, RelativeMultiHeadAttention\n",
    "from dnabert import DnaBertModel, DnaBertPretrainingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4d97ce-9467-4370-926d-01d899ff9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 512\n",
    "batch_size = global_batch_size // 4\n",
    "min_length = 65\n",
    "max_length = 250\n",
    "mask_ratio = 0.15\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "feedforward_dim = 2048\n",
    "stack = 8\n",
    "kmer = 6\n",
    "kmer_stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27a83cc-f8be-410c-a8cc-1fbea2ba3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = DnaBertPretrainingModel(\n",
    "    DnaBertModel(\n",
    "        TransformerEncoder(\n",
    "            TransformerEncoderBlock(\n",
    "                RelativeMultiHeadAttention(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    max_length=max_length,\n",
    "                ),\n",
    "                feedforward_dim=feedforward_dim,\n",
    "            ),\n",
    "            num_layers=stack\n",
    "        ),\n",
    "        kmer=kmer,\n",
    "        kmer_stride=kmer_stride\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd769192-8efa-4ac5-bc08-f156c311dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "def transform(entry):\n",
    "    # Sequence\n",
    "    sequence = entry.sequence\n",
    "    minlen, maxlen = min(min_length, len(sequence)), min(max_length, len(sequence))\n",
    "    length = torch.randint(minlen, maxlen, size=(1,)).item()\n",
    "    offset = torch.randint(0, len(sequence) - length + 1, size=(1,)).item()\n",
    "    sequence = torch.tensor(list(bert.base.vocabulary(bert.base.tokenizer(sequence[offset:offset+length]))))\n",
    "\n",
    "    # Masking\n",
    "    mask_length = torch.randint(1, int(len(sequence)*mask_ratio) + 1, size=(1,)).item()\n",
    "    mask_offset = torch.randint(0, len(sequence) - mask_length + 1, size=(1,)).item()\n",
    "    masked_tokens = sequence[mask_offset:mask_offset+mask_length].clone()\n",
    "    sequence[mask_offset:mask_offset+mask_length] = bert.base.vocabulary[\"[MASK]\"]\n",
    "\n",
    "    # Padding\n",
    "    sequence = F.pad(sequence, (0, max_length - len(sequence) + 1), value=bert.base.vocabulary[\"[PAD]\"])\n",
    "    return sequence, masked_tokens\n",
    "\n",
    "def collate(entries):\n",
    "    sequences, masked_tokens = zip(*entries)\n",
    "    return (torch.stack(sequences), torch.cat(masked_tokens)), None, None\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset([\n",
    "    SequenceDataset(\n",
    "        \"/home/data2/deepdna/datasets/silva_nr99_filtered_515f_806r/sequences.fasta.db/\",\n",
    "        transform=transform\n",
    "    )\n",
    "])\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_dataset, num_samples=num_workers*batch_size, replacement=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fa10f2-6ef0-4b8f-835e-fd900f80aa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03195476531982422"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "it = iter(train_loader)\n",
    "\n",
    "t = time.time()\n",
    "batch = next(it)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66903aee-3c57-49f5-b208-3e25b592e51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[   4,    4,    4,  ...,    0,    0,    0],\n",
       "          [2716, 2657, 2421,  ...,    0,    0,    0],\n",
       "          [3232,  625, 2488,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [3207,  528, 2098,  ...,    0,    0,    0],\n",
       "          [2779, 2909, 3431,  ...,    0,    0,    0],\n",
       "          [   1,    1,    1,  ...,    0,    0,    0]]),\n",
       "  tensor([2720, 2673, 2485,  ..., 2722, 2681, 2517])),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0872f45e-1865-49f1-aee7-4386abfc4f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msirdavidludwig\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dwl2x/.cache/wandb/wandb/run-20240914_152850-gqj3qtza</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sirdavidludwig/dnabert-768/runs/gqj3qtza' target=\"_blank\">earthy-energy-2</a></strong> to <a href='https://wandb.ai/sirdavidludwig/dnabert-768' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sirdavidludwig/dnabert-768' target=\"_blank\">https://wandb.ai/sirdavidludwig/dnabert-768</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sirdavidludwig/dnabert-768/runs/gqj3qtza' target=\"_blank\">https://wandb.ai/sirdavidludwig/dnabert-768/runs/gqj3qtza</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = None\n",
    "run = wandb.init(project=\"dnabert-768\")\n",
    "logger = WandbLogger(project=\"dnabert-768\", log_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1080cc-d167-4514-98ed-7fccbac47dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 2711/-2 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">8/8</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:05 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.00it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: qtza</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 2711/-2 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m8/8\u001b[0m \u001b[38;5;245m0:00:05 • 0:00:00\u001b[0m \u001b[38;5;249m0.00it/s\u001b[0m \u001b[37mv_num: qtza\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_steps=100000,\n",
    "    callbacks=[RichProgressBar(refresh_rate=10)],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=50,\n",
    "    accumulate_grad_batches=global_batch_size//batch_size)\n",
    "trainer.fit(\n",
    "    model=bert,\n",
    "    train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe57515-424d-4ea1-990f-de94ef4a3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert, \"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7999f26-e881-4c70-b5db-416ddc7e92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wandb.Artifact(f\"dnabert.{embed_dim}d.silva\", type=\"model\")\n",
    "a.add_file(\"./model.pt\")\n",
    "run.log_artifact(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b020e0-cf81-4360-8be3-c6c55bce25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Qiime2)",
   "language": "python",
   "name": "qiime2-amplicon-2024.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
